MainThread 2017-01-13 18:00:27 [ERROR]:string indices must be integers, not str |code info--> database_utils.py-transform_format_string(169)
MainThread 2017-01-13 18:01:19 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:02:02 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:04:27 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:05:22 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:10:06 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:10:26 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:11:17 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:12:01 [DEBUG]:数据库连接失败 :  |code info--> database_utils.py-get_connect(46)
MainThread 2017-01-13 18:14:25 [INFO]:database connection success |code info--> database_utils.py-get_connect(43)
MainThread 2017-01-13 18:17:40 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:18:41 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:18:41 [ERROR]: |code info--> database_utils.py-select_multi_data(65)
MainThread 2017-01-13 18:18:41 [ERROR]:select error, sql is select did, tablename, colname, coltype, tcomment, is_partition_field, colstatus, create_date, update_date from table_dictionary where tablename = "idl_limao_mobile_relation_raw_agg
" |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-13 18:19:53 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:19:53 [ERROR]: |code info--> database_utils.py-select_multi_data(65)
MainThread 2017-01-13 18:19:53 [ERROR]:select error, sql is select did, tablename, colname, coltype, tcomment, is_partition_field, colstatus, create_date, update_date from table_dictionary where tablename = "idl_limao_mobile_relation_raw_agg
" |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-13 18:20:27 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:21:15 [ERROR]: |code info--> database_utils.py-select_multi_data(65)
MainThread 2017-01-13 18:21:15 [ERROR]:select error, sql is select did, tablename, colname, coltype, tcomment, is_partition_field, colstatus, create_date, update_date from table_dictionary where tablename = "idl_limao_mobile_relation_raw_agg
" |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-13 18:21:44 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:23:07 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:26:51 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:27:02 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:27:33 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:29:21 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:29:21 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:29:21 [ERROR]: |code info--> database_utils.py-batch_modify_database(114)
MainThread 2017-01-13 18:29:21 [ERROR]:execute sql fail : sql = insert into table_dictionary(did, tablename, colname, coltype, tcomment, is_partition_field, colstatus, create_date, update_date) values (null, "idl_limao_mobile_relation_raw_agg
", "tag_psb", "array<string>", "", "False", "use", now(), null) |code info--> database_utils.py-batch_modify_database(115)
MainThread 2017-01-13 18:29:50 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:30:16 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:30:16 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:31:05 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:31:05 [ERROR]:data_list is none |code info--> database_utils.py-transform_format_string(176)
MainThread 2017-01-13 18:31:10 [ERROR]:parameter is none |code info--> database_utils.py-batch_modify_database(119)
MainThread 2017-01-13 18:32:17 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:32:17 [ERROR]:data_list is none |code info--> database_utils.py-transform_format_string(176)
MainThread 2017-01-13 18:32:20 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:32:20 [ERROR]:parameter is none |code info--> database_utils.py-batch_modify_database(119)
MainThread 2017-01-13 18:41:27 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:41:27 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:42:03 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:42:03 [ERROR]:data_list is none |code info--> database_utils.py-transform_format_string(176)
MainThread 2017-01-13 18:42:03 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 18:42:03 [ERROR]:parameter is none |code info--> database_utils.py-batch_modify_database(119)
MainThread 2017-01-13 18:42:03 [INFO]:database connection success |code info--> database_utils.py-get_connect(44)
MainThread 2017-01-13 19:15:02 [INFO]:monitor day : 2017-01-13 |code info--> monitor_daily_table.py-run_history_data(120)
MainThread 2017-01-13 19:15:02 [DEBUG]:monitor script file list : adl_limao_commodity_token_agg.sql,adl_limao_recommended_cid_agg.sql,adl_limao_user_recommended_agg.sql,adl_person_tag_agg.sql,ald_limao_receiver_agg.sql,idl_address_main_dim.sql,idl_limao_address_agg.sql,idl_limao_cidprice_dim.sql,idl_limao_cid_dim.sql,idl_limao_commodity_token_agg.sql,idl_limao_info_dim.sql,idl_limao_mobile_name_agg.sql,idl_limao_receiver_agg.sql,idl_limao_supple_info_agg.sql,idl_limao_user_cid_agg.sql,idl_limao_user_title_agg.sql,idl_moblie_name_agg.sql,idl_titel_token_log.sql,idl_user_mobile_initial_agg.sql |code info--> utils.py-file_list(25)
MainThread 2017-01-13 19:15:02 [INFO]:obtain sql script file list success |code info--> monitor_daily_table.py-batch_run_hive(23)
MainThread 2017-01-13 19:15:02 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:02 [INFO]:remain job size : 19 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:03 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:03 [INFO]:starting hive thread : adl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:03 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_commodity_token_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_commodity_token_agg' as tablename,
    ds
from 
    adl_limao_commodity_token_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:03 [INFO]:job done : adl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:03 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:03 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:03 [INFO]:remain job size : 18 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:04 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-13 19:15:04 [INFO]:starting hive thread : adl_limao_recommended_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-13 19:15:05 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_recommended_cid_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_recommended_cid_agg' as tablename,
    ds
from 
    adl_limao_recommended_cid_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-13 19:15:05 [INFO]:job done : adl_limao_recommended_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-13 19:15:05 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:05 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:05 [INFO]:remain job size : 17 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:06 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-13 19:15:06 [INFO]:starting hive thread : adl_limao_user_recommended_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-13 19:15:06 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_user_recommended_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_user_recommended_agg' as tablename,
    ds
from 
    adl_limao_user_recommended_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-13 19:15:06 [INFO]:job done : adl_limao_user_recommended_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-13 19:15:06 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:06 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:06 [INFO]:remain job size : 16 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:07 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_person_tag_agg.sql_2017-01-13 2017-01-13 19:15:07 [INFO]:starting hive thread : adl_person_tag_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_person_tag_agg.sql_2017-01-13 2017-01-13 19:15:07 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_person_tag_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_person_tag_agg' as tablename,
    ds
from 
    adl_person_tag_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_person_tag_agg.sql_2017-01-13 2017-01-13 19:15:07 [INFO]:job done : adl_person_tag_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_person_tag_agg.sql_2017-01-13 2017-01-13 19:15:07 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:07 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:07 [INFO]:remain job size : 15 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:08 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:08 [INFO]:starting hive thread : ald_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:08 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='ald_limao_receiver_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'ald_limao_receiver_agg' as tablename,
    ds
from 
    ald_limao_receiver_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:08 [INFO]:job done : ald_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:08 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:08 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:08 [INFO]:remain job size : 14 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:09 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_address_main_dim.sql_2017-01-13 2017-01-13 19:15:09 [INFO]:starting hive thread : idl_address_main_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_address_main_dim.sql_2017-01-13 2017-01-13 19:15:09 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_address_main_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_address_main_dim' as tablename,
    ds
from 
    idl_address_main_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_address_main_dim.sql_2017-01-13 2017-01-13 19:15:09 [INFO]:job done : idl_address_main_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_address_main_dim.sql_2017-01-13 2017-01-13 19:15:09 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:09 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:09 [INFO]:remain job size : 13 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:10 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_address_agg.sql_2017-01-13 2017-01-13 19:15:10 [INFO]:starting hive thread : idl_limao_address_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_address_agg.sql_2017-01-13 2017-01-13 19:15:10 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_address_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_address_agg' as tablename,
    ds
from 
    idl_limao_address_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_address_agg.sql_2017-01-13 2017-01-13 19:15:10 [INFO]:job done : idl_limao_address_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_address_agg.sql_2017-01-13 2017-01-13 19:15:10 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:10 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:10 [INFO]:remain job size : 12 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:11 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-13 19:15:11 [INFO]:starting hive thread : idl_limao_cidprice_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-13 19:15:11 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cidprice_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cidprice_dim' as tablename,
    ds
from 
    idl_limao_cidprice_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-13 19:15:11 [INFO]:job done : idl_limao_cidprice_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-13 19:15:11 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:11 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:11 [INFO]:remain job size : 11 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:12 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_cid_dim.sql_2017-01-13 2017-01-13 19:15:12 [INFO]:starting hive thread : idl_limao_cid_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_cid_dim.sql_2017-01-13 2017-01-13 19:15:12 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cid_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cid_dim' as tablename,
    ds
from 
    idl_limao_cid_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cid_dim.sql_2017-01-13 2017-01-13 19:15:12 [INFO]:job done : idl_limao_cid_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_cid_dim.sql_2017-01-13 2017-01-13 19:15:12 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:12 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:12 [INFO]:remain job size : 10 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:13 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:13 [INFO]:starting hive thread : idl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:13 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_commodity_token_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_commodity_token_agg' as tablename,
    ds
from 
    idl_limao_commodity_token_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:13 [INFO]:job done : idl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-13 19:15:13 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:13 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:13 [INFO]:remain job size : 9 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:14 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_info_dim.sql_2017-01-13 2017-01-13 19:15:14 [INFO]:starting hive thread : idl_limao_info_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_info_dim.sql_2017-01-13 2017-01-13 19:15:14 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_info_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_info_dim' as tablename,
    ds
from 
    idl_limao_info_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_info_dim.sql_2017-01-13 2017-01-13 19:15:14 [INFO]:job done : idl_limao_info_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_info_dim.sql_2017-01-13 2017-01-13 19:15:14 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:14 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:14 [INFO]:remain job size : 8 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:15 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-13 19:15:15 [INFO]:starting hive thread : idl_limao_mobile_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-13 19:15:15 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_mobile_name_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_mobile_name_agg' as tablename,
    ds
from 
    idl_limao_mobile_name_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-13 19:15:15 [INFO]:job done : idl_limao_mobile_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-13 19:15:15 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:15 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:15 [INFO]:remain job size : 7 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:16 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:16 [INFO]:starting hive thread : idl_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:16 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_receiver_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_receiver_agg' as tablename,
    ds
from 
    idl_limao_receiver_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:16 [INFO]:job done : idl_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-13 19:15:16 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:16 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:16 [INFO]:remain job size : 6 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:17 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-13 19:15:17 [INFO]:starting hive thread : idl_limao_supple_info_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-13 19:15:17 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_supple_info_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_supple_info_agg' as tablename,
    ds
from 
    idl_limao_supple_info_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-13 19:15:17 [INFO]:job done : idl_limao_supple_info_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-13 19:15:17 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:17 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:17 [INFO]:remain job size : 5 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:18 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-13 19:15:18 [INFO]:starting hive thread : idl_limao_user_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-13 19:15:18 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_cid_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_cid_agg' as tablename,
    ds
from 
    idl_limao_user_cid_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-13 19:15:18 [INFO]:job done : idl_limao_user_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-13 19:15:18 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:18 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:18 [INFO]:remain job size : 4 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:19 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-13 19:15:19 [INFO]:starting hive thread : idl_limao_user_title_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-13 19:15:19 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_title_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_title_agg' as tablename,
    ds
from 
    idl_limao_user_title_agg
where ds = '2017-01-13'
group by ds;
 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-13 19:15:19 [INFO]:job done : idl_limao_user_title_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-13 19:15:19 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:19 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:19 [INFO]:remain job size : 3 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:20 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_moblie_name_agg.sql_2017-01-13 2017-01-13 19:15:20 [INFO]:starting hive thread : idl_moblie_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_moblie_name_agg.sql_2017-01-13 2017-01-13 19:15:20 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_moblie_name_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_moblie_name_agg' as tablename,
    ds
from 
    idl_moblie_name_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_moblie_name_agg.sql_2017-01-13 2017-01-13 19:15:20 [INFO]:job done : idl_moblie_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_moblie_name_agg.sql_2017-01-13 2017-01-13 19:15:20 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:20 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:20 [INFO]:remain job size : 2 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:21 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_titel_token_log.sql_2017-01-13 2017-01-13 19:15:21 [INFO]:starting hive thread : idl_titel_token_log.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
idl_titel_token_log.sql_2017-01-13 2017-01-13 19:15:21 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_titel_token_log',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_titel_token_log' as tablename,
    ds
from 
    idl_titel_token_log
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_titel_token_log.sql_2017-01-13 2017-01-13 19:15:21 [INFO]:job done : idl_titel_token_log.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_titel_token_log.sql_2017-01-13 2017-01-13 19:15:21 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:21 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:21 [INFO]:remain job size : 1 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:22 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-13 19:15:22 [INFO]:starting hive thread : idl_user_mobile_initial_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
MainThread 2017-01-13 19:15:22 [INFO]:monitor day : 2017-01-14 |code info--> monitor_daily_table.py-run_history_data(120)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-13 19:15:22 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_user_mobile_initial_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_user_mobile_initial_agg' as tablename,
    ds
from 
    idl_user_mobile_initial_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-13 19:15:22 [INFO]:job done : idl_user_mobile_initial_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-13 19:15:22 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:22 [DEBUG]:monitor script file list : adl_limao_commodity_token_agg.sql,adl_limao_recommended_cid_agg.sql,adl_limao_user_recommended_agg.sql,adl_person_tag_agg.sql,ald_limao_receiver_agg.sql,idl_address_main_dim.sql,idl_limao_address_agg.sql,idl_limao_cidprice_dim.sql,idl_limao_cid_dim.sql,idl_limao_commodity_token_agg.sql,idl_limao_info_dim.sql,idl_limao_mobile_name_agg.sql,idl_limao_receiver_agg.sql,idl_limao_supple_info_agg.sql,idl_limao_user_cid_agg.sql,idl_limao_user_title_agg.sql,idl_moblie_name_agg.sql,idl_titel_token_log.sql,idl_user_mobile_initial_agg.sql |code info--> utils.py-file_list(25)
MainThread 2017-01-13 19:15:22 [INFO]:obtain sql script file list success |code info--> monitor_daily_table.py-batch_run_hive(23)
MainThread 2017-01-13 19:15:22 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:22 [INFO]:remain job size : 19 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:23 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:23 [INFO]:starting hive thread : adl_limao_commodity_token_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:23 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_commodity_token_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_commodity_token_agg' as tablename,
    ds
from 
    adl_limao_commodity_token_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:23 [INFO]:job done : adl_limao_commodity_token_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:23 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:23 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:23 [INFO]:remain job size : 18 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:24 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_recommended_cid_agg.sql_2017-01-14 2017-01-13 19:15:24 [INFO]:starting hive thread : adl_limao_recommended_cid_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_recommended_cid_agg.sql_2017-01-14 2017-01-13 19:15:24 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_recommended_cid_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_recommended_cid_agg' as tablename,
    ds
from 
    adl_limao_recommended_cid_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_recommended_cid_agg.sql_2017-01-14 2017-01-13 19:15:24 [INFO]:job done : adl_limao_recommended_cid_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_recommended_cid_agg.sql_2017-01-14 2017-01-13 19:15:24 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:24 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:24 [INFO]:remain job size : 17 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:25 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_user_recommended_agg.sql_2017-01-14 2017-01-13 19:15:25 [INFO]:starting hive thread : adl_limao_user_recommended_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_user_recommended_agg.sql_2017-01-14 2017-01-13 19:15:25 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_user_recommended_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_user_recommended_agg' as tablename,
    ds
from 
    adl_limao_user_recommended_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_user_recommended_agg.sql_2017-01-14 2017-01-13 19:15:25 [INFO]:job done : adl_limao_user_recommended_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_user_recommended_agg.sql_2017-01-14 2017-01-13 19:15:25 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:25 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:25 [INFO]:remain job size : 16 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:26 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_person_tag_agg.sql_2017-01-14 2017-01-13 19:15:26 [INFO]:starting hive thread : adl_person_tag_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
adl_person_tag_agg.sql_2017-01-14 2017-01-13 19:15:26 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_person_tag_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_person_tag_agg' as tablename,
    ds
from 
    adl_person_tag_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_person_tag_agg.sql_2017-01-14 2017-01-13 19:15:26 [INFO]:job done : adl_person_tag_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
adl_person_tag_agg.sql_2017-01-14 2017-01-13 19:15:26 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:26 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:26 [INFO]:remain job size : 15 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:27 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
ald_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:27 [INFO]:starting hive thread : ald_limao_receiver_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
ald_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:27 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='ald_limao_receiver_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'ald_limao_receiver_agg' as tablename,
    ds
from 
    ald_limao_receiver_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
ald_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:27 [INFO]:job done : ald_limao_receiver_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
ald_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:27 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:27 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:27 [INFO]:remain job size : 14 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:28 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_address_main_dim.sql_2017-01-14 2017-01-13 19:15:28 [INFO]:starting hive thread : idl_address_main_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_address_main_dim.sql_2017-01-14 2017-01-13 19:15:28 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_address_main_dim',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_address_main_dim' as tablename,
    ds
from 
    idl_address_main_dim
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_address_main_dim.sql_2017-01-14 2017-01-13 19:15:28 [INFO]:job done : idl_address_main_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_address_main_dim.sql_2017-01-14 2017-01-13 19:15:28 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:28 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:28 [INFO]:remain job size : 13 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:29 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_address_agg.sql_2017-01-14 2017-01-13 19:15:29 [INFO]:starting hive thread : idl_limao_address_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_address_agg.sql_2017-01-14 2017-01-13 19:15:29 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_address_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_address_agg' as tablename,
    ds
from 
    idl_limao_address_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_address_agg.sql_2017-01-14 2017-01-13 19:15:29 [INFO]:job done : idl_limao_address_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_address_agg.sql_2017-01-14 2017-01-13 19:15:29 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:29 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:29 [INFO]:remain job size : 12 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:30 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_cidprice_dim.sql_2017-01-14 2017-01-13 19:15:30 [INFO]:starting hive thread : idl_limao_cidprice_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_cidprice_dim.sql_2017-01-14 2017-01-13 19:15:30 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cidprice_dim',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cidprice_dim' as tablename,
    ds
from 
    idl_limao_cidprice_dim
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cidprice_dim.sql_2017-01-14 2017-01-13 19:15:30 [INFO]:job done : idl_limao_cidprice_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_cidprice_dim.sql_2017-01-14 2017-01-13 19:15:30 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:30 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:30 [INFO]:remain job size : 11 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:31 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_cid_dim.sql_2017-01-14 2017-01-13 19:15:31 [INFO]:starting hive thread : idl_limao_cid_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_cid_dim.sql_2017-01-14 2017-01-13 19:15:31 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cid_dim',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cid_dim' as tablename,
    ds
from 
    idl_limao_cid_dim
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cid_dim.sql_2017-01-14 2017-01-13 19:15:31 [INFO]:job done : idl_limao_cid_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_cid_dim.sql_2017-01-14 2017-01-13 19:15:31 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:31 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:31 [INFO]:remain job size : 10 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:32 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:32 [INFO]:starting hive thread : idl_limao_commodity_token_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:32 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_commodity_token_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_commodity_token_agg' as tablename,
    ds
from 
    idl_limao_commodity_token_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:32 [INFO]:job done : idl_limao_commodity_token_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_commodity_token_agg.sql_2017-01-14 2017-01-13 19:15:32 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:32 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:32 [INFO]:remain job size : 9 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:33 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_info_dim.sql_2017-01-14 2017-01-13 19:15:33 [INFO]:starting hive thread : idl_limao_info_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_info_dim.sql_2017-01-14 2017-01-13 19:15:33 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_info_dim',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_info_dim' as tablename,
    ds
from 
    idl_limao_info_dim
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_info_dim.sql_2017-01-14 2017-01-13 19:15:33 [INFO]:job done : idl_limao_info_dim.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_info_dim.sql_2017-01-14 2017-01-13 19:15:33 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:33 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:33 [INFO]:remain job size : 8 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:34 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_mobile_name_agg.sql_2017-01-14 2017-01-13 19:15:34 [INFO]:starting hive thread : idl_limao_mobile_name_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_mobile_name_agg.sql_2017-01-14 2017-01-13 19:15:34 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_mobile_name_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_mobile_name_agg' as tablename,
    ds
from 
    idl_limao_mobile_name_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_mobile_name_agg.sql_2017-01-14 2017-01-13 19:15:34 [INFO]:job done : idl_limao_mobile_name_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_mobile_name_agg.sql_2017-01-14 2017-01-13 19:15:34 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:34 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:34 [INFO]:remain job size : 7 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:35 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:35 [INFO]:starting hive thread : idl_limao_receiver_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:35 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_receiver_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_receiver_agg' as tablename,
    ds
from 
    idl_limao_receiver_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:35 [INFO]:job done : idl_limao_receiver_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_receiver_agg.sql_2017-01-14 2017-01-13 19:15:35 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:35 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:35 [INFO]:remain job size : 6 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:36 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_supple_info_agg.sql_2017-01-14 2017-01-13 19:15:36 [INFO]:starting hive thread : idl_limao_supple_info_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_supple_info_agg.sql_2017-01-14 2017-01-13 19:15:36 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_supple_info_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_supple_info_agg' as tablename,
    ds
from 
    idl_limao_supple_info_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_supple_info_agg.sql_2017-01-14 2017-01-13 19:15:36 [INFO]:job done : idl_limao_supple_info_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_supple_info_agg.sql_2017-01-14 2017-01-13 19:15:36 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:36 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:36 [INFO]:remain job size : 5 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:37 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_user_cid_agg.sql_2017-01-14 2017-01-13 19:15:37 [INFO]:starting hive thread : idl_limao_user_cid_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_user_cid_agg.sql_2017-01-14 2017-01-13 19:15:37 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_cid_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_cid_agg' as tablename,
    ds
from 
    idl_limao_user_cid_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_cid_agg.sql_2017-01-14 2017-01-13 19:15:37 [INFO]:job done : idl_limao_user_cid_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_user_cid_agg.sql_2017-01-14 2017-01-13 19:15:37 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:37 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:37 [INFO]:remain job size : 4 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:38 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_user_title_agg.sql_2017-01-14 2017-01-13 19:15:38 [INFO]:starting hive thread : idl_limao_user_title_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_user_title_agg.sql_2017-01-14 2017-01-13 19:15:38 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_title_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_title_agg' as tablename,
    ds
from 
    idl_limao_user_title_agg
where ds = '2017-01-14'
group by ds;
 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_title_agg.sql_2017-01-14 2017-01-13 19:15:38 [INFO]:job done : idl_limao_user_title_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_user_title_agg.sql_2017-01-14 2017-01-13 19:15:38 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:38 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:38 [INFO]:remain job size : 3 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:39 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_moblie_name_agg.sql_2017-01-14 2017-01-13 19:15:39 [INFO]:starting hive thread : idl_moblie_name_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_moblie_name_agg.sql_2017-01-14 2017-01-13 19:15:39 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_moblie_name_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_moblie_name_agg' as tablename,
    ds
from 
    idl_moblie_name_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_moblie_name_agg.sql_2017-01-14 2017-01-13 19:15:39 [INFO]:job done : idl_moblie_name_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_moblie_name_agg.sql_2017-01-14 2017-01-13 19:15:39 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:39 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:39 [INFO]:remain job size : 2 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:40 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_titel_token_log.sql_2017-01-14 2017-01-13 19:15:40 [INFO]:starting hive thread : idl_titel_token_log.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_titel_token_log.sql_2017-01-14 2017-01-13 19:15:40 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_titel_token_log',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_titel_token_log' as tablename,
    ds
from 
    idl_titel_token_log
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_titel_token_log.sql_2017-01-14 2017-01-13 19:15:40 [INFO]:job done : idl_titel_token_log.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_titel_token_log.sql_2017-01-14 2017-01-13 19:15:40 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:15:40 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:15:40 [INFO]:remain job size : 1 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:15:41 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_user_mobile_initial_agg.sql_2017-01-14 2017-01-13 19:15:41 [INFO]:starting hive thread : idl_user_mobile_initial_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(82)
idl_user_mobile_initial_agg.sql_2017-01-14 2017-01-13 19:15:41 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_user_mobile_initial_agg',ds='2017-01-14');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_user_mobile_initial_agg' as tablename,
    ds
from 
    idl_user_mobile_initial_agg
where ds = '2017-01-14'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_user_mobile_initial_agg.sql_2017-01-14 2017-01-13 19:15:41 [INFO]:job done : idl_user_mobile_initial_agg.sql_2017-01-14 |code info--> monitor_daily_table.py-run_hive(102)
idl_user_mobile_initial_agg.sql_2017-01-14 2017-01-13 19:15:41 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:16:28 [INFO]:sql path : resource/daily-monitor/ |code info--> monitor_daily_table.py-run_schedule_data(130)
MainThread 2017-01-13 19:17:11 [INFO]:sql path : resource/daily-monitor/ |code info--> monitor_daily_table.py-run_schedule_data(130)
MainThread 2017-01-13 19:17:11 [DEBUG]:monitor script file list : adl_limao_commodity_token_agg.sql,adl_limao_recommended_cid_agg.sql,adl_limao_user_recommended_agg.sql,adl_person_tag_agg.sql,ald_limao_receiver_agg.sql,idl_address_main_dim.sql,idl_limao_address_agg.sql,idl_limao_cidprice_dim.sql,idl_limao_cid_dim.sql,idl_limao_commodity_token_agg.sql,idl_limao_info_dim.sql,idl_limao_mobile_name_agg.sql,idl_limao_receiver_agg.sql,idl_limao_supple_info_agg.sql,idl_limao_user_cid_agg.sql,idl_limao_user_title_agg.sql,idl_moblie_name_agg.sql,idl_titel_token_log.sql,idl_user_mobile_initial_agg.sql |code info--> utils.py-file_list(25)
MainThread 2017-01-13 19:17:11 [INFO]:obtain sql script file list success |code info--> monitor_daily_table.py-batch_run_hive(23)
MainThread 2017-01-13 19:17:11 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:11 [INFO]:remain job size : 19 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:12 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:12 [INFO]:starting hive thread : adl_limao_commodity_token_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:12 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_commodity_token_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_commodity_token_agg' as tablename,
    ds
from 
    adl_limao_commodity_token_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:12 [INFO]:job done : adl_limao_commodity_token_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:12 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:12 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:12 [INFO]:remain job size : 18 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:13 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_recommended_cid_agg.sql_2017-01-12 2017-01-13 19:17:13 [INFO]:starting hive thread : adl_limao_recommended_cid_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_recommended_cid_agg.sql_2017-01-12 2017-01-13 19:17:13 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_recommended_cid_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_recommended_cid_agg' as tablename,
    ds
from 
    adl_limao_recommended_cid_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_recommended_cid_agg.sql_2017-01-12 2017-01-13 19:17:13 [INFO]:job done : adl_limao_recommended_cid_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_recommended_cid_agg.sql_2017-01-12 2017-01-13 19:17:13 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:13 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:13 [INFO]:remain job size : 17 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:14 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_user_recommended_agg.sql_2017-01-12 2017-01-13 19:17:14 [INFO]:starting hive thread : adl_limao_user_recommended_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_user_recommended_agg.sql_2017-01-12 2017-01-13 19:17:14 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_user_recommended_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_user_recommended_agg' as tablename,
    ds
from 
    adl_limao_user_recommended_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_user_recommended_agg.sql_2017-01-12 2017-01-13 19:17:14 [INFO]:job done : adl_limao_user_recommended_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_user_recommended_agg.sql_2017-01-12 2017-01-13 19:17:14 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:14 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:14 [INFO]:remain job size : 16 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:15 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_person_tag_agg.sql_2017-01-12 2017-01-13 19:17:15 [INFO]:starting hive thread : adl_person_tag_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
adl_person_tag_agg.sql_2017-01-12 2017-01-13 19:17:15 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_person_tag_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_person_tag_agg' as tablename,
    ds
from 
    adl_person_tag_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_person_tag_agg.sql_2017-01-12 2017-01-13 19:17:15 [INFO]:job done : adl_person_tag_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
adl_person_tag_agg.sql_2017-01-12 2017-01-13 19:17:15 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:15 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:15 [INFO]:remain job size : 15 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:16 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
ald_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:16 [INFO]:starting hive thread : ald_limao_receiver_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
ald_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:16 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='ald_limao_receiver_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'ald_limao_receiver_agg' as tablename,
    ds
from 
    ald_limao_receiver_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
ald_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:16 [INFO]:job done : ald_limao_receiver_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
ald_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:16 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:16 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:16 [INFO]:remain job size : 14 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:17 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_address_main_dim.sql_2017-01-12 2017-01-13 19:17:17 [INFO]:starting hive thread : idl_address_main_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_address_main_dim.sql_2017-01-12 2017-01-13 19:17:17 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_address_main_dim',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_address_main_dim' as tablename,
    ds
from 
    idl_address_main_dim
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_address_main_dim.sql_2017-01-12 2017-01-13 19:17:17 [INFO]:job done : idl_address_main_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_address_main_dim.sql_2017-01-12 2017-01-13 19:17:17 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:17 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:17 [INFO]:remain job size : 13 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:18 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_address_agg.sql_2017-01-12 2017-01-13 19:17:18 [INFO]:starting hive thread : idl_limao_address_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_address_agg.sql_2017-01-12 2017-01-13 19:17:18 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_address_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_address_agg' as tablename,
    ds
from 
    idl_limao_address_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_address_agg.sql_2017-01-12 2017-01-13 19:17:18 [INFO]:job done : idl_limao_address_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_address_agg.sql_2017-01-12 2017-01-13 19:17:18 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:18 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:18 [INFO]:remain job size : 12 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:19 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_cidprice_dim.sql_2017-01-12 2017-01-13 19:17:19 [INFO]:starting hive thread : idl_limao_cidprice_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_cidprice_dim.sql_2017-01-12 2017-01-13 19:17:19 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cidprice_dim',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cidprice_dim' as tablename,
    ds
from 
    idl_limao_cidprice_dim
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cidprice_dim.sql_2017-01-12 2017-01-13 19:17:19 [INFO]:job done : idl_limao_cidprice_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_cidprice_dim.sql_2017-01-12 2017-01-13 19:17:19 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:19 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:19 [INFO]:remain job size : 11 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:20 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_cid_dim.sql_2017-01-12 2017-01-13 19:17:20 [INFO]:starting hive thread : idl_limao_cid_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_cid_dim.sql_2017-01-12 2017-01-13 19:17:20 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cid_dim',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cid_dim' as tablename,
    ds
from 
    idl_limao_cid_dim
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cid_dim.sql_2017-01-12 2017-01-13 19:17:20 [INFO]:job done : idl_limao_cid_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_cid_dim.sql_2017-01-12 2017-01-13 19:17:20 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:20 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:20 [INFO]:remain job size : 10 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:21 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:21 [INFO]:starting hive thread : idl_limao_commodity_token_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:21 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_commodity_token_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_commodity_token_agg' as tablename,
    ds
from 
    idl_limao_commodity_token_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:21 [INFO]:job done : idl_limao_commodity_token_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_commodity_token_agg.sql_2017-01-12 2017-01-13 19:17:21 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:21 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:21 [INFO]:remain job size : 9 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:22 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_info_dim.sql_2017-01-12 2017-01-13 19:17:22 [INFO]:starting hive thread : idl_limao_info_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_info_dim.sql_2017-01-12 2017-01-13 19:17:22 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_info_dim',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_info_dim' as tablename,
    ds
from 
    idl_limao_info_dim
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_info_dim.sql_2017-01-12 2017-01-13 19:17:22 [INFO]:job done : idl_limao_info_dim.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_info_dim.sql_2017-01-12 2017-01-13 19:17:22 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:22 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:22 [INFO]:remain job size : 8 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:23 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_mobile_name_agg.sql_2017-01-12 2017-01-13 19:17:23 [INFO]:starting hive thread : idl_limao_mobile_name_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_mobile_name_agg.sql_2017-01-12 2017-01-13 19:17:23 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_mobile_name_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_mobile_name_agg' as tablename,
    ds
from 
    idl_limao_mobile_name_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_mobile_name_agg.sql_2017-01-12 2017-01-13 19:17:23 [INFO]:job done : idl_limao_mobile_name_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_mobile_name_agg.sql_2017-01-12 2017-01-13 19:17:23 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:23 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:23 [INFO]:remain job size : 7 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:24 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:24 [INFO]:starting hive thread : idl_limao_receiver_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:24 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_receiver_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_receiver_agg' as tablename,
    ds
from 
    idl_limao_receiver_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:24 [INFO]:job done : idl_limao_receiver_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_receiver_agg.sql_2017-01-12 2017-01-13 19:17:24 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:24 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:24 [INFO]:remain job size : 6 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:25 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_supple_info_agg.sql_2017-01-12 2017-01-13 19:17:25 [INFO]:starting hive thread : idl_limao_supple_info_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_supple_info_agg.sql_2017-01-12 2017-01-13 19:17:25 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_supple_info_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_supple_info_agg' as tablename,
    ds
from 
    idl_limao_supple_info_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_supple_info_agg.sql_2017-01-12 2017-01-13 19:17:25 [INFO]:job done : idl_limao_supple_info_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_supple_info_agg.sql_2017-01-12 2017-01-13 19:17:25 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:25 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:25 [INFO]:remain job size : 5 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:26 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_user_cid_agg.sql_2017-01-12 2017-01-13 19:17:26 [INFO]:starting hive thread : idl_limao_user_cid_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_user_cid_agg.sql_2017-01-12 2017-01-13 19:17:26 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_cid_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_cid_agg' as tablename,
    ds
from 
    idl_limao_user_cid_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_cid_agg.sql_2017-01-12 2017-01-13 19:17:26 [INFO]:job done : idl_limao_user_cid_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_user_cid_agg.sql_2017-01-12 2017-01-13 19:17:26 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:26 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:26 [INFO]:remain job size : 4 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:27 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_limao_user_title_agg.sql_2017-01-12 2017-01-13 19:17:27 [INFO]:starting hive thread : idl_limao_user_title_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_limao_user_title_agg.sql_2017-01-12 2017-01-13 19:17:27 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_title_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_title_agg' as tablename,
    ds
from 
    idl_limao_user_title_agg
where ds = '2017-01-12'
group by ds;
 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_title_agg.sql_2017-01-12 2017-01-13 19:17:27 [INFO]:job done : idl_limao_user_title_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_limao_user_title_agg.sql_2017-01-12 2017-01-13 19:17:27 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:27 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:27 [INFO]:remain job size : 3 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:28 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_moblie_name_agg.sql_2017-01-12 2017-01-13 19:17:28 [INFO]:starting hive thread : idl_moblie_name_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_moblie_name_agg.sql_2017-01-12 2017-01-13 19:17:28 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_moblie_name_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_moblie_name_agg' as tablename,
    ds
from 
    idl_moblie_name_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_moblie_name_agg.sql_2017-01-12 2017-01-13 19:17:28 [INFO]:job done : idl_moblie_name_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_moblie_name_agg.sql_2017-01-12 2017-01-13 19:17:28 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:28 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:28 [INFO]:remain job size : 2 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:29 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_titel_token_log.sql_2017-01-12 2017-01-13 19:17:29 [INFO]:starting hive thread : idl_titel_token_log.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_titel_token_log.sql_2017-01-12 2017-01-13 19:17:29 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_titel_token_log',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_titel_token_log' as tablename,
    ds
from 
    idl_titel_token_log
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_titel_token_log.sql_2017-01-12 2017-01-13 19:17:29 [INFO]:job done : idl_titel_token_log.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_titel_token_log.sql_2017-01-12 2017-01-13 19:17:29 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-13 19:17:29 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-13 19:17:29 [INFO]:remain job size : 1 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-13 19:17:30 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
idl_user_mobile_initial_agg.sql_2017-01-12 2017-01-13 19:17:30 [INFO]:starting hive thread : idl_user_mobile_initial_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(82)
idl_user_mobile_initial_agg.sql_2017-01-12 2017-01-13 19:17:30 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_user_mobile_initial_agg',ds='2017-01-12');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_user_mobile_initial_agg' as tablename,
    ds
from 
    idl_user_mobile_initial_agg
where ds = '2017-01-12'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
idl_user_mobile_initial_agg.sql_2017-01-12 2017-01-13 19:17:30 [INFO]:job done : idl_user_mobile_initial_agg.sql_2017-01-12 |code info--> monitor_daily_table.py-run_hive(102)
idl_user_mobile_initial_agg.sql_2017-01-12 2017-01-13 19:17:30 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-14 15:30:09 [INFO]:sql path : resource/daily-monitor/ |code info--> monitor_daily_table.py-run_schedule_data(130)
MainThread 2017-01-14 15:30:09 [DEBUG]:monitor script file list : adl_limao_commodity_token_agg.sql,adl_limao_recommended_cid_agg.sql,adl_limao_user_recommended_agg.sql,adl_person_tag_agg.sql,ald_limao_receiver_agg.sql,idl_address_main_dim.sql,idl_limao_address_agg.sql,idl_limao_cidprice_dim.sql,idl_limao_cid_dim.sql,idl_limao_commodity_token_agg.sql,idl_limao_info_dim.sql,idl_limao_mobile_name_agg.sql,idl_limao_receiver_agg.sql,idl_limao_supple_info_agg.sql,idl_limao_user_cid_agg.sql,idl_limao_user_title_agg.sql,idl_moblie_name_agg.sql,idl_titel_token_log.sql,idl_user_mobile_initial_agg.sql |code info--> utils.py-file_list(25)
MainThread 2017-01-14 15:30:09 [INFO]:obtain sql script file list success |code info--> monitor_daily_table.py-batch_run_hive(23)
MainThread 2017-01-14 15:30:09 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-14 15:30:09 [INFO]:remain job size : 19 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-14 15:30:10 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 15:30:10 [INFO]:starting hive thread : adl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 15:30:10 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_commodity_token_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_commodity_token_agg' as tablename,
    ds
from 
    adl_limao_commodity_token_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 15:30:10 [INFO]:job done : adl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 15:30:10 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-14 15:30:10 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-14 15:30:10 [INFO]:remain job size : 18 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-14 15:30:11 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 15:30:11 [INFO]:starting hive thread : adl_limao_recommended_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 15:30:11 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_recommended_cid_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_recommended_cid_agg' as tablename,
    ds
from 
    adl_limao_recommended_cid_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 15:30:11 [INFO]:job done : adl_limao_recommended_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 15:30:11 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-14 15:30:11 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-14 15:30:11 [INFO]:remain job size : 17 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-14 15:30:12 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(64)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 15:30:12 [INFO]:starting hive thread : adl_limao_user_recommended_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(82)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 15:30:12 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_user_recommended_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_user_recommended_agg' as tablename,
    ds
from 
    adl_limao_user_recommended_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 15:30:12 [INFO]:job done : adl_limao_user_recommended_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(102)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 15:30:12 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(103)
MainThread 2017-01-14 15:30:12 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(52)
MainThread 2017-01-14 15:30:12 [INFO]:remain job size : 16 |code info--> monitor_daily_table.py-batch_run_hive(57)
MainThread 2017-01-14 17:27:15 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-14 17:27:24 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-14 17:27:54 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-14 17:39:48 [INFO]:sql path : resource/daily-monitor/ |code info--> monitor_daily_table.py-run_schedule_data(131)
MainThread 2017-01-14 17:39:48 [DEBUG]:monitor script file list : adl_limao_commodity_token_agg.sql,adl_limao_recommended_cid_agg.sql,adl_limao_user_recommended_agg.sql,adl_person_tag_agg.sql,ald_limao_receiver_agg.sql,idl_address_main_dim.sql,idl_limao_address_agg.sql,idl_limao_cidprice_dim.sql,idl_limao_cid_dim.sql,idl_limao_commodity_token_agg.sql,idl_limao_info_dim.sql,idl_limao_mobile_name_agg.sql,idl_limao_receiver_agg.sql,idl_limao_supple_info_agg.sql,idl_limao_user_cid_agg.sql,idl_limao_user_title_agg.sql,idl_moblie_name_agg.sql,idl_titel_token_log.sql,idl_user_mobile_initial_agg.sql |code info--> utils.py-file_list(25)
MainThread 2017-01-14 17:39:48 [INFO]:obtain sql script file list success |code info--> monitor_daily_table.py-batch_run_hive(24)
MainThread 2017-01-14 17:39:48 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:48 [INFO]:remain job size : 19 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:49 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:49 [INFO]:starting hive thread : adl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:49 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_commodity_token_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_commodity_token_agg' as tablename,
    ds
from 
    adl_limao_commodity_token_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:49 [INFO]:job done : adl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
adl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:49 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:49 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:49 [INFO]:remain job size : 18 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:50 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 17:39:50 [INFO]:starting hive thread : adl_limao_recommended_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 17:39:50 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_recommended_cid_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_recommended_cid_agg' as tablename,
    ds
from 
    adl_limao_recommended_cid_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 17:39:50 [INFO]:job done : adl_limao_recommended_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
adl_limao_recommended_cid_agg.sql_2017-01-13 2017-01-14 17:39:50 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:50 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:50 [INFO]:remain job size : 17 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:51 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 17:39:51 [INFO]:starting hive thread : adl_limao_user_recommended_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 17:39:51 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_limao_user_recommended_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_limao_user_recommended_agg' as tablename,
    ds
from 
    adl_limao_user_recommended_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 17:39:51 [INFO]:job done : adl_limao_user_recommended_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
adl_limao_user_recommended_agg.sql_2017-01-13 2017-01-14 17:39:51 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:51 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:51 [INFO]:remain job size : 16 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:52 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
adl_person_tag_agg.sql_2017-01-13 2017-01-14 17:39:52 [INFO]:starting hive thread : adl_person_tag_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
adl_person_tag_agg.sql_2017-01-13 2017-01-14 17:39:52 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='adl_person_tag_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'adl_person_tag_agg' as tablename,
    ds
from 
    adl_person_tag_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
adl_person_tag_agg.sql_2017-01-13 2017-01-14 17:39:52 [INFO]:job done : adl_person_tag_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
adl_person_tag_agg.sql_2017-01-13 2017-01-14 17:39:52 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:52 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:52 [INFO]:remain job size : 15 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:53 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:39:53 [INFO]:starting hive thread : ald_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:39:53 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='ald_limao_receiver_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'ald_limao_receiver_agg' as tablename,
    ds
from 
    ald_limao_receiver_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:39:53 [INFO]:job done : ald_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
ald_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:39:53 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:53 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:53 [INFO]:remain job size : 14 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:54 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_address_main_dim.sql_2017-01-13 2017-01-14 17:39:54 [INFO]:starting hive thread : idl_address_main_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_address_main_dim.sql_2017-01-13 2017-01-14 17:39:54 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_address_main_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_address_main_dim' as tablename,
    ds
from 
    idl_address_main_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_address_main_dim.sql_2017-01-13 2017-01-14 17:39:54 [INFO]:job done : idl_address_main_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_address_main_dim.sql_2017-01-13 2017-01-14 17:39:54 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:54 [INFO]:connect_num : 2 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:54 [INFO]:remain job size : 13 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:55 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_address_agg.sql_2017-01-13 2017-01-14 17:39:55 [INFO]:starting hive thread : idl_limao_address_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_address_agg.sql_2017-01-13 2017-01-14 17:39:55 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_address_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_address_agg' as tablename,
    ds
from 
    idl_limao_address_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_address_agg.sql_2017-01-13 2017-01-14 17:39:55 [INFO]:job done : idl_limao_address_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_address_agg.sql_2017-01-13 2017-01-14 17:39:55 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:55 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:55 [INFO]:remain job size : 12 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:56 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-14 17:39:56 [INFO]:starting hive thread : idl_limao_cidprice_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-14 17:39:56 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cidprice_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cidprice_dim' as tablename,
    ds
from 
    idl_limao_cidprice_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-14 17:39:56 [INFO]:job done : idl_limao_cidprice_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_cidprice_dim.sql_2017-01-13 2017-01-14 17:39:56 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:56 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:56 [INFO]:remain job size : 11 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:57 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_cid_dim.sql_2017-01-13 2017-01-14 17:39:57 [INFO]:starting hive thread : idl_limao_cid_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_cid_dim.sql_2017-01-13 2017-01-14 17:39:57 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_cid_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_cid_dim' as tablename,
    ds
from 
    idl_limao_cid_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_cid_dim.sql_2017-01-13 2017-01-14 17:39:57 [INFO]:job done : idl_limao_cid_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_cid_dim.sql_2017-01-13 2017-01-14 17:39:57 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:57 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:57 [INFO]:remain job size : 10 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:58 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:58 [INFO]:starting hive thread : idl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:58 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_commodity_token_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_commodity_token_agg' as tablename,
    ds
from 
    idl_limao_commodity_token_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:58 [INFO]:job done : idl_limao_commodity_token_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_commodity_token_agg.sql_2017-01-13 2017-01-14 17:39:58 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:58 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:58 [INFO]:remain job size : 9 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:39:59 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_info_dim.sql_2017-01-13 2017-01-14 17:39:59 [INFO]:starting hive thread : idl_limao_info_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_info_dim.sql_2017-01-13 2017-01-14 17:39:59 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_info_dim',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_info_dim' as tablename,
    ds
from 
    idl_limao_info_dim
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_info_dim.sql_2017-01-13 2017-01-14 17:39:59 [INFO]:job done : idl_limao_info_dim.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_info_dim.sql_2017-01-13 2017-01-14 17:39:59 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:39:59 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:39:59 [INFO]:remain job size : 8 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:00 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-14 17:40:00 [INFO]:starting hive thread : idl_limao_mobile_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-14 17:40:00 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_mobile_name_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_mobile_name_agg' as tablename,
    ds
from 
    idl_limao_mobile_name_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-14 17:40:00 [INFO]:job done : idl_limao_mobile_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_mobile_name_agg.sql_2017-01-13 2017-01-14 17:40:00 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:00 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:00 [INFO]:remain job size : 7 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:01 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:40:01 [INFO]:starting hive thread : idl_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:40:01 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_receiver_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_receiver_agg' as tablename,
    ds
from 
    idl_limao_receiver_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:40:01 [INFO]:job done : idl_limao_receiver_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_receiver_agg.sql_2017-01-13 2017-01-14 17:40:01 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:01 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:01 [INFO]:remain job size : 6 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:02 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-14 17:40:02 [INFO]:starting hive thread : idl_limao_supple_info_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-14 17:40:02 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_supple_info_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_supple_info_agg' as tablename,
    ds
from 
    idl_limao_supple_info_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-14 17:40:02 [INFO]:job done : idl_limao_supple_info_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_supple_info_agg.sql_2017-01-13 2017-01-14 17:40:02 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:02 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:02 [INFO]:remain job size : 5 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:03 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-14 17:40:03 [INFO]:starting hive thread : idl_limao_user_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-14 17:40:03 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_cid_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_cid_agg' as tablename,
    ds
from 
    idl_limao_user_cid_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-14 17:40:03 [INFO]:job done : idl_limao_user_cid_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_user_cid_agg.sql_2017-01-13 2017-01-14 17:40:03 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:03 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:03 [INFO]:remain job size : 4 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:04 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-14 17:40:04 [INFO]:starting hive thread : idl_limao_user_title_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-14 17:40:04 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_limao_user_title_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_limao_user_title_agg' as tablename,
    ds
from 
    idl_limao_user_title_agg
where ds = '2017-01-13'
group by ds;
 |code info--> monitor_daily_table.py-run_hive(84)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-14 17:40:04 [INFO]:job done : idl_limao_user_title_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_limao_user_title_agg.sql_2017-01-13 2017-01-14 17:40:04 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:04 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:04 [INFO]:remain job size : 3 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:05 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_moblie_name_agg.sql_2017-01-13 2017-01-14 17:40:05 [INFO]:starting hive thread : idl_moblie_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_moblie_name_agg.sql_2017-01-13 2017-01-14 17:40:05 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_moblie_name_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_moblie_name_agg' as tablename,
    ds
from 
    idl_moblie_name_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_moblie_name_agg.sql_2017-01-13 2017-01-14 17:40:05 [INFO]:job done : idl_moblie_name_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_moblie_name_agg.sql_2017-01-13 2017-01-14 17:40:05 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:05 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:05 [INFO]:remain job size : 2 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:06 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_titel_token_log.sql_2017-01-13 2017-01-14 17:40:06 [INFO]:starting hive thread : idl_titel_token_log.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_titel_token_log.sql_2017-01-13 2017-01-14 17:40:06 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_titel_token_log',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_titel_token_log' as tablename,
    ds
from 
    idl_titel_token_log
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_titel_token_log.sql_2017-01-13 2017-01-14 17:40:06 [INFO]:job done : idl_titel_token_log.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_titel_token_log.sql_2017-01-13 2017-01-14 17:40:06 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 17:40:06 [INFO]:connect_num : 1 |code info--> monitor_daily_table.py-batch_run_hive(53)
MainThread 2017-01-14 17:40:06 [INFO]:remain job size : 1 |code info--> monitor_daily_table.py-batch_run_hive(58)
MainThread 2017-01-14 17:40:07 [INFO]:current running job : MainThread |code info--> monitor_daily_table.py-batch_run_hive(65)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-14 17:40:07 [INFO]:starting hive thread : idl_user_mobile_initial_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(83)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-14 17:40:07 [INFO]:hive sql : use leesdata;
ALTER TABLE idl_datasource_count_ft DROP PARTITION(tablename='idl_user_mobile_initial_agg',ds='2017-01-13');
insert into table idl_datasource_count_ft partition(tablename, ds)
select 
    'all_count' as colname, 
    count(1) as data_count,
    'idl_user_mobile_initial_agg' as tablename,
    ds
from 
    idl_user_mobile_initial_agg
where ds = '2017-01-13'
group by ds; |code info--> monitor_daily_table.py-run_hive(84)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-14 17:40:07 [INFO]:job done : idl_user_mobile_initial_agg.sql_2017-01-13 |code info--> monitor_daily_table.py-run_hive(103)
idl_user_mobile_initial_agg.sql_2017-01-13 2017-01-14 17:40:07 [INFO]:job running time:1970-01-01 08:00:00 |code info--> monitor_daily_table.py-run_hive(104)
MainThread 2017-01-14 18:34:53 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 09:57:41 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(132)
MainThread 2017-01-15 09:57:41 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(128)
MainThread 2017-01-15 09:57:41 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(128)
MainThread 2017-01-15 09:57:41 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(189)
MainThread 2017-01-15 09:58:12 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(132)
MainThread 2017-01-15 09:58:12 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(128)
MainThread 2017-01-15 09:58:12 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(128)
MainThread 2017-01-15 09:58:12 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(189)
MainThread 2017-01-15 09:58:12 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(189)
MainThread 2017-01-15 09:58:12 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 09:58:12 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(189)
MainThread 2017-01-15 09:58:12 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(189)
MainThread 2017-01-15 09:58:12 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 09:58:13 [INFO]:monitor success |code info--> monitor_daily_table.py-run_monitor(153)
MainThread 2017-01-15 12:06:17 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:06:17 [ERROR]: |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-15 12:06:17 [ERROR]:select error, sql is SELECT job_id, job_name, param, table_name, table_type, check_status FROM result_target_log where check_status = 0 limist 2; |code info--> database_utils.py-select_multi_data(67)
MainThread 2017-01-15 12:06:17 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:06:50 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:06:50 [ERROR]: |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-15 12:06:50 [ERROR]:select error, sql is SELECT job_id, job_name, param, table_name, table_type, check_status FROM result_target_log where check_status = 0 limist 2; |code info--> database_utils.py-select_multi_data(67)
MainThread 2017-01-15 12:06:50 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:07:20 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:07:20 [ERROR]: |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-15 12:07:20 [ERROR]:select error, sql is SELECT job_id, job_name, param, table_name, table_type, check_status FROM result_target_log where check_status = 0 limist 2; |code info--> database_utils.py-select_multi_data(67)
MainThread 2017-01-15 12:07:20 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:07:27 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:07:27 [ERROR]: |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-15 12:07:27 [ERROR]:select error, sql is SELECT job_id, job_name, param, table_name, table_type, check_status FROM result_target_log where check_status = 0 limist 2; |code info--> database_utils.py-select_multi_data(67)
MainThread 2017-01-15 12:07:27 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:07:37 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:07:37 [ERROR]: |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-15 12:07:37 [ERROR]:select error, sql is SELECT job_id, job_name, param, table_name, table_type, check_status FROM result_target_log where check_status = 0 limist 2; |code info--> database_utils.py-select_multi_data(67)
MainThread 2017-01-15 12:07:37 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:08:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:09:05 [ERROR]: |code info--> database_utils.py-select_multi_data(66)
MainThread 2017-01-15 12:10:07 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:10:07 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:10:46 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:10:50 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:11:29 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:12:06 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:13:28 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:13:29 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:13:29 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:13:42 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:13:43 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:15:25 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:15:25 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:15:54 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:15:54 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:15:54 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:15:54 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:15:54 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:15:55 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:15:55 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:15:55 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:15:55 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:15:55 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:16:42 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:16:42 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:17:01 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:17:01 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:17:01 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:17:01 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:17:01 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:17:01 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:17:02 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:17:02 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:17:02 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:17:02 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:17:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:17:59 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:17:59 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:17:59 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:17:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
monitor_thread_1 2017-01-15 12:17:59 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:17:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:17:59 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:18:35 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:18:35 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:19:03 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:19:03 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:19:20 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:19:20 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:19:39 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:19:39 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:19:44 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:20:03 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:20:06 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
monitor_thread_1 2017-01-15 12:20:15 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:20:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:20:59 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:20:59 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:20:59 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:20:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
monitor_thread_1 2017-01-15 12:20:59 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:20:59 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:20:59 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:20:59 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:20:59 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:21:00 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
monitor_thread_2 2017-01-15 12:21:00 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:21:23 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:21:23 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:21:37 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:21:37 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:21:37 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:21:37 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:21:37 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
monitor_thread_1 2017-01-15 12:21:37 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:21:37 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:21:37 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:21:37 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:21:37 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:21:37 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
monitor_thread_2 2017-01-15 12:21:38 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:23:03 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:23:03 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:23:03 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:23:06 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:23:06 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:23:17 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:23:39 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:23:39 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:23:50 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:23:50 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:23:50 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:24:03 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:24:03 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:24:05 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:34:13 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:34:13 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:34:13 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:34:13 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:34:13 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:34:13 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:34:13 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:34:13 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:34:13 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:34:13 [INFO]:monitor success |code info--> monitor_daily_table.py-run_monitor(175)
MainThread 2017-01-15 12:34:13 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:34:13 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:34:13 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:34:13 [INFO]:monitor success |code info--> monitor_daily_table.py-run_monitor(175)
MainThread 2017-01-15 12:35:19 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:35:19 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:35:19 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:35:19 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:35:19 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:35:23 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:35:27 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:35:54 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:35:55 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:35:57 [INFO]:monitor success |code info--> monitor_daily_table.py-run_monitor(172)
MainThread 2017-01-15 12:36:08 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:36:18 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:36:20 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:36:21 [INFO]:monitor success |code info--> monitor_daily_table.py-run_monitor(172)
MainThread 2017-01-15 12:37:40 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:37:40 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:37:40 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:37:40 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:37:40 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:37:42 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:37:55 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:37:59 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:39:17 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:39:55 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:39:55 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:39:55 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:39:55 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:39:55 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:39:55 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:39:55 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:39:56 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:41:07 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:41:07 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:41:07 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:41:07 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:41:07 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:41:07 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:41:07 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:41:11 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:41:11 [ERROR]: |code info--> database_utils.py-batch_modify_database(116)
MainThread 2017-01-15 12:41:11 [ERROR]:execute sql fail : sql = update result_targetsdf_log set result = 10, updatedt = now(), check_status = 2 where job_id = "T7795753989178458" and table_name = "idl_address_pois_log";update result_target_log set result = 10, updatedt = now(), check_status = 2 where job_id = "T7795753989178458" and table_name = "idl_address_pois_log"; |code info--> database_utils.py-batch_modify_database(117)
MainThread 2017-01-15 12:41:30 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:41:30 [INFO]:select multi-data success |code info--> database_utils.py-select_multi_data(69)
MainThread 2017-01-15 12:41:30 [INFO]:convert data success |code info--> monitor_daily_table.py-get_target_list(130)
MainThread 2017-01-15 12:41:30 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:41:30 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:41:30 [INFO]:monitor start |code info--> monitor_daily_table.py-run_monitor(135)
MainThread 2017-01-15 12:41:30 [INFO]:transform string format success |code info--> database_utils.py-transform_format_string(190)
MainThread 2017-01-15 12:41:32 [INFO]:database connection success |code info--> database_utils.py-get_connect(45)
MainThread 2017-01-15 12:41:32 [ERROR]: |code info--> database_utils.py-batch_modify_database(116)
MainThread 2017-01-15 12:41:32 [ERROR]:execute sql fail : sql = updatedsd result_targetsdf_log set result = 10, updatedt = now(), check_status = 2 where job_id = "T7795753989178458" and table_name = "idl_address_pois_log";update result_target_log set result = 10, updatedt = now(), check_status = 2 where job_id = "T7795753989178458" and table_name = "idl_address_pois_log"; |code info--> database_utils.py-batch_modify_database(117)
